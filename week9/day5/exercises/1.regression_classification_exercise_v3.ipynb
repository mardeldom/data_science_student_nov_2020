{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2e3c1e8da6e821b0193601da5a0e541c0efda9704da45b6b820f5f4c317f2def"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añade al archivo \"5.regression_classification_exercise_v2\" del CW9D2/exercises/, todos los algoritmos que hemos visto. Estos son:\n",
    "\n",
    "### Regresión:\n",
    "\n",
    "- Linear Regression\n",
    "- SVM (versión regresión SVR)\n",
    "- Polinominal Regression\n",
    "- Random Forest (versión regresión)\n",
    "\n",
    "### Clasificación:\n",
    "\n",
    "- SVM (versión regresión SVC)\n",
    "- Knn\n",
    "- Random Forest (versión clasificación)\n",
    "- Xgboost (si todo OK)\n",
    "- Logistic regression\n",
    "\n",
    "Haz que se puedan ejecutar de forma genérica para varias features de los algoritmos. Por ejemplo, que se ejecute con \"param\" para diferentes grados del polinomio y para usar diferentes kernels en SVM. \n",
    "\n",
    "Utiliza Grid Search sacar el mejor posible de los algoritmos elegidos con diferentes parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn import utils\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn import pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "source": [
    "# Regresión"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_model(option_user, params=None):\n",
    "    params_model = {\"kernel_reg\": params, \"degree\": params, \"kernel_class\": params, \"k\": params}\n",
    "    choose= input(\"Elige entre un modelo de regresión (1) o clasificación(2)\")\n",
    "    if choose == 1: # Regression model\n",
    "        if option_user== 1:\n",
    "            model = linear_model.LinearRegression()\n",
    "        elif option_user== 2:\n",
    "            params= input(\"Elige el kernel a utilizar linear, poly, rbf\")\n",
    "            model= SVR(kernel= params_model[\"kernel_reg\"])\n",
    "        elif option_user== 3:\n",
    "            model = RandomForestRegressor()\n",
    "        elif option_user == 4:\n",
    "            params = int(input(\"Introduce un degree para el polinomio\"))\n",
    "            model = PolynomialFeatures(params_model[\"degree\"])\n",
    "        else: \n",
    "            print(\"Elige el modelo correcto\")\n",
    "    elif choose_model == 2: #Classification model\n",
    "        if option_user== 5:\n",
    "            model = linear_model.LogisticRegression()\n",
    "        elif option_user== 6:\n",
    "            params= input(\"Elige el kernel a utilizar linear, sigmoid, rbf\")\n",
    "            model= SVC(kernel= params_model[\"kernel_class\"])\n",
    "        elif option_user== 7:\n",
    "            model = RandomForestClassifier()\n",
    "        elif option_user == 8:\n",
    "            params= int(input(\"Introduce un valor para k\"))\n",
    "            model = KNeighborsClassifier(n_neighbors=params_model[\"k\"])\n",
    "        elif option_user == 9:\n",
    "            model= XGBClassifier()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, df, target_name):\n",
    "    df= df._get_numeric_data()\n",
    "    X= df.drop(columns= target_name)    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    model_trained= model.fit(X_train, y_train)\n",
    "    accuracy= model.score(X_test, y_test)\n",
    "    return model_trained, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    md = choose_model(option_user=int(input(\"Introduce el módelo que quieres utilizar, puedes elegir entre regresión lineal(1), SVR(2), Random Forest Regression(3), Polynomial(4), Regresión logística(5), SVC(6), Random Forest Classifier(7), Knn(8), XGBClassifier(9)\")))\n",
    "    path= input(\"Introduce la URL de tu CSV:\")\n",
    "    data = pd.read_csv(path)\n",
    "    target_name = input(\"Introduce el nombre de la columna target\")\n",
    "    model_trained= train_model(model=md, df=data, target_name=target_name)\n",
    "    return model_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_data(model_trained, to_pred):\n",
    "    y_pred = model_trained.predict(to_pred)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crear función GridSearch\n",
    "def grid_search():\n",
    "    pipe = Pipeline(steps=[('classifier', RandomForestClassifier())])\n",
    "\n",
    "\n",
    "logistic_params = {\n",
    "    'classifier': [LogisticRegression()],\n",
    "    \n",
    "\n",
    "random_forest_params = {\n",
    "    'classifier': [RandomForestClassifier()]\n",
    "    }\n",
    "\n",
    "svm_params = {\n",
    "    'classifier': [SVC()]\n",
    "    }\n",
    "knn_params = {\n",
    "    \"classifier\": [KNeighborsClassifier()]\n",
    "}\n",
    "\n",
    "XG= {\n",
    "    \"classifier\": [XGBClassifier()]\n",
    "}\n",
    "\n",
    "\n",
    "# Create space of candidate learning algorithms and their hyperparameters\n",
    "search_space = [\n",
    "    logistic_params,\n",
    "    random_forest_params,\n",
    "    svm_params,\n",
    "    knn_params, \n",
    "    XG\n",
    "    ]\n",
    "    clf = GridSearchCV(estimator=pipe, param_grid=search_space, verbose=0, n_jobs=-1, scoring=\"recall\")\n",
    "\n",
    "    # Fit grid search\n",
    "    best_model = clf.fit(X_train, y_train)\n",
    "    # View best model\n",
    "    separator = \"\\n############################\\n\"\n",
    "    print(separator)\n",
    "    print(\"best estimator:\", best_model.best_estimator_.get_params()['classifier'])\n",
    "    print(separator)\n",
    "    print(\"clf.best_params_\", clf.best_params_)\n",
    "    print(separator)\n",
    "    # Mean cross-validated score of the best_estimator\n",
    "    print(\"clf.best_score\", clf.best_score_)"
   ]
  }
 ]
}